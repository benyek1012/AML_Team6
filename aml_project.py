# -*- coding: utf-8 -*-
"""AML_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XlIxWtSpB8i5d0ucOsQnhWXp3PKXN6St

### 1. Preparing the data

Import the used library for training dataset and connect to Google Drive
"""

import os
import zipfile
from os import listdir
from matplotlib import image
import tensorflow as tf
import numpy as np
import random
import matplotlib.pyplot as plt

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.constraints import unit_norm
from tensorflow.keras import regularizers
from tensorflow.keras.metrics import top_k_categorical_accuracy

from google.colab import drive
drive.mount('Mydrive')

pwd

"""#### **Unzip the dataset and define few preprocessing steps**"""

local_zip = '/content/Mydrive/MyDrive/Colab Notebooks/AML_Project/train.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp')
zip_ref.close()

local_zip = '/content/Mydrive/MyDrive/Colab Notebooks/AML_Project/test.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp')
zip_ref.close()

"""Define a normalization function"""

IMG_SIZE_ARRAY = (160, 160)
def _normalize_img(img):
    img = tf.cast(img, tf.float32)/255.   # All images will be rescaled by 1./255
    img = tf.image.resize(img, IMG_SIZE_ARRAY, method= 'bilinear')
    return (img)

"""#### **Load the train and test data in a list**
This method will load all the data into memory (RAM) before feeding it into the classifier for training. This is only possible when our dataset is small.

Create array list storing image paths and labels
"""

train_info = '/content/Mydrive/MyDrive/Colab Notebooks/AML_Project/train.txt'

with open(train_info) as txt_file:
  lines = [x.strip() for x in txt_file.readlines()]

def TrainImageAndLabel():
  data = []
  for x in lines:
      temp = x.split(' ')
      img_data = image.imread('/tmp/' + temp[0])
      normalized_img = _normalize_img(img_data)
      data.append([normalized_img, int(temp[1])])
      print('> loaded %s %s with label %s' % (temp[0], normalized_img.shape, temp[1]))
  return data

train_data = TrainImageAndLabel()

test_info = '/content/Mydrive/MyDrive/Colab Notebooks/AML_Project/groundtruth.txt'

with open(test_info) as txt_file:
  lines = [x.strip() for x in txt_file.readlines()]

def TestImageAndLabel():
  data = []
  for x in lines:
      temp = x.split(' ')
      img_data = image.imread('/tmp/' + temp[0])
      normalized_img = _normalize_img(img_data)
      data.append([normalized_img, int(temp[1])])
      print('> loaded %s %s with label %s' % (temp[0], normalized_img.shape, temp[1]))
  return data

test_data = TestImageAndLabel()

x_train = []
y_train = []
for x,y in train_data:
  x_train.append(x)
  y_train.append(y)

x_test = []
y_test = []
for x,y in test_data:
  x_test.append(x)
  y_test.append(y)

# Convert Python List, "train_loaded_images" to numpy Arrays, "X_train"
x_train = np.array(x_train)
# Convert Python List, "test_loaded_images" to numpy Arrays, "X_test"
x_test = np.array(x_test)

# Convert Python List, "Y_train" to numpy Arrays, "Y_train"
y_train = np.array(y_train)
# Convert Python List, "Y_test" to numpy Arrays, "Y_test"
y_test = np.array(y_test)

random_value = random.randint(0, len(x_train))

plt.imshow(x_train[random_value, :])
plt.show()
print(y_train[random_value])

train_label = np.unique(y_train)
no_classes = train_label.size
print(no_classes)

def changeLabel():
  i=0;
  label = []
  for temp in train_label:
    label.append([temp,i])
    i += 1
  return label

new_label = changeLabel()

print(new_label)

i =0;
for label in y_train:
  for x,y in new_label:
    if(label == x):
      y_train[i] = y
      break
  i += 1

print(y_train)

j =0;
for label in y_test:
  for x,y in new_label:
    if(label == x):
      y_test[j] = y
      break
  j += 1

print(y_test)

print("Shape of X_train : ", x_train.shape)
print("Shape of X_test : ", x_test.shape)

print("Shape of Y_train : ", y_train.shape)
print("Shape of Y_test : ", y_test.shape)

"""### CNN architecture"""

# Create the base model from the pre-trained model MobileNet V2
IMG_SHAPE = IMG_SIZE_ARRAY + (3,)

data_augmentation = tf.keras.Sequential([
  tf.keras.layers.RandomFlip('horizontal'),
  tf.keras.layers.RandomRotation(0.2),
])

# Normalization of preprocessing input 

def preprocess_input_mobile_net():
  return tf.keras.applications.mobilenet_v2.preprocess_input

preprocess_input = preprocess_input_mobile_net()

def model_mobile_net():
  return tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
            include_top=False, weights='imagenet')
  
base_model = model_mobile_net()

base_model.trainable = False

# Let's take a look at the base model architecture
base_model.summary()

global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

prediction_layer = tf.keras.layers.Dense(no_classes)

inputs = tf.keras.Input(shape=(160, 160, 3))
x = data_augmentation(inputs)
x = preprocess_input(x)
x = base_model(inputs, training=False)
x = global_average_layer(x)
x = tf.keras.layers.Dropout(0.2)(x)
x = Dense(1024, activation='relu')(x)
x = tf.keras.layers.Dropout(0.2)(x)
x = Dense(1024, activation='relu')(x)
outputs = prediction_layer(x)
model = tf.keras.Model(inputs, outputs)

base_learning_rate = 0.0001
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.summary()

len(model.trainable_variables)

loss0, accuracy0 = model.evaluate(x_test, y_test)

initial_epochs = 20

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)

history = model.fit(x_train, y_train,
                    epochs=initial_epochs,
                    batch_size = 32, validation_data=(x_test,y_test),
                    callbacks=[callback])

#Get the training model parameters
history.history.keys()

#Generate a accuracy graph
plt.plot(history.epoch, history.history.get("accuracy"), label = "train_accuracy")
plt.plot(history.epoch, history.history.get("val_accuracy"), label = "val_accuracy")
plt.legend()

#Generate a loss graph
plt.plot(history.epoch, history.history.get("loss"), label = "train_loss")
plt.plot(history.epoch, history.history.get("val_loss"), label = "val_loss")
plt.legend()

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

base_model.trainable = True

# Let's take a look to see how many layers are in the base model
print("Number of layers in the base model: ", len(base_model.layers))

# Fine-tune from this layer onwards
fine_tune_at = 100

# Freeze all the layers before the `fine_tune_at` layer
for layer in base_model.layers[:fine_tune_at]:
  layer.trainable = False

model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.summary()

fine_tune_epochs = 20
total_epochs =  initial_epochs + fine_tune_epochs

history_fine = model.fit(x_train, y_train,
                    epochs=total_epochs,
                    initial_epoch=history.epoch[-1],
                    batch_size = 32,
                    validation_data=(x_test,y_test),  callbacks=[callback])

acc += history_fine.history['accuracy']
val_acc += history_fine.history['val_accuracy']

loss += history_fine.history['loss']
val_loss += history_fine.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.ylim([0.8, 1])
plt.plot([initial_epochs-1,initial_epochs-1],
          plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.ylim([0, 1.0])
plt.plot([initial_epochs-1,initial_epochs-1],
         plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

# plot results
# accuracy
plt.figure(figsize=(10, 16))
plt.rcParams['figure.figsize'] = [16, 9]
plt.rcParams['font.size'] = 14
plt.rcParams['axes.grid'] = True
plt.rcParams['figure.facecolor'] = 'white'
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.title(f'\nTraining and Validation Accuracy. \nTrain Accuracy:{str(acc[-1])}\nValidation Accuracy: {str(val_acc[-1])}')

# loss
plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.title(f'Training and Validation Loss. \nTrain Loss:{str(loss[-1])}\nValidation Loss: {str(val_loss[-1])}')
plt.xlabel('epoch')
plt.tight_layout(pad=3.0)
plt.show()

accuracy_score = model.evaluate(x_test, y_test)
print(accuracy_score)
print("Accuracy: {:.4f}%".format(accuracy_score[1] * 100))
 
print("Loss: ",accuracy_score[0])

model.save("AML_MobileNet.h5")